# -*- coding: utf-8 -*-
"""RAG- Assment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/113_td7eMiIV_XmPjp0EVHaQqXPmSwpI1
"""

pip install PyMuPDF

from google.colab import files
uploaded = files.upload()  # This will prompt you to upload a file

import nltk

# Manually specify the correct directory and download the 'punkt' resource.
nltk.data.path.append('/root/nltk_data')  # Ensure the correct path

# Download the correct resource for sentence tokenization
nltk.download('punkt', download_dir='/root/nltk_data')

!pip install spacy
!python -m spacy download en_core_web_sm

import re
import fitz  # PyMuPDF
from google.colab import files

# Step 1: Upload the PDF file
uploaded = files.upload()  # This will allow you to upload the file manually

# Step 2: Extract text from the uploaded PDF file
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)  # Open the PDF file
    text = ""
    for page in doc:
        text += page.get_text()  # Extract text from each page
    return text

# Step 3: Tokenize text into sentences using regex
def chunk_text(document):
    # Use regex to split text into sentences
    sentences = re.split(r'(?<=[.!?]) +', document)  # Split by punctuation marks followed by a space
    return sentences

# Get the uploaded file name (assuming you uploaded 'Frequently-asked-questions-2022-15092022.pdf')
pdf_path = list(uploaded.keys())[0]  # Get the uploaded file's name

# Step 4: Extract text from the uploaded PDF
document_text = extract_text_from_pdf(pdf_path)

# Step 5: Chunk the extracted text into sentences using regex
chunks = chunk_text(document_text)

# Print the first few chunks to verify
print(chunks[:5])  # Print the first 5 chunks to verify

!pip install faiss-cpu

!pip install sentence-transformers

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# Initialize the Sentence Transformer model (you can replace this with any pre-trained model)
model = SentenceTransformer('all-MiniLM-L6-v2')  # This is a light model; you can use more powerful models if needed.

# Vectorize the chunks
embeddings = model.encode(chunks)

# FAISS index setup
dim = embeddings.shape[1]  # Dimensionality of the vectors
index = faiss.IndexFlatL2(dim)  # L2 distance metric for cosine similarity
index.add(np.array(embeddings))  # Add the vectors to the index

# Example of a query
query = "Tell me about Airtel's address"
query_embedding = model.encode([query])

# Search for the top 3 most relevant chunks
D, I = index.search(np.array(query_embedding), 3)

# Print the most relevant chunks
print("Top 3 relevant chunks:")
for idx in I[0]:
    print(chunks[idx])  # Retrieve the corresponding text chunks

!pip install openai

!pip install --upgrade openai

!pip install transformers
!pip install torch



from transformers import pipeline

# Initialize the Question-Answering pipeline with a pre-trained model
qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

# Example function to generate answers using the pipeline
def get_answer_from_local_model(context, question):
    result = qa_pipeline(question=question, context=context)
    return result['answer']

# Example question
question = "What is Airtel's address?"

# Use the top retrieved chunk as context (you can adjust this based on FAISS)
relevant_text = chunks[I[0][0]]  # Use the top chunk retrieved by FAISS

# Get the answer from the model
answer = get_answer_from_local_model(relevant_text, question)
print("Generated Answer:", answer)

import re

def simple_answer(query, context):
    # Simple keyword-based search for an answer
    query = query.lower()
    context = context.lower()

    # Find relevant sentences by looking for matching keywords in the context
    sentences = context.split(". ")
    relevant_sentences = [sentence for sentence in sentences if re.search(r"\b" + re.escape(query) + r"\b", sentence)]

    if relevant_sentences:
        return relevant_sentences[0]  # Return the first relevant sentence
    else:
        return "Sorry, I couldn't find an answer."

# Example question
question = "What is Airtel's address?"

# Use the top retrieved chunk as context (you can adjust this based on FAISS)
relevant_text = chunks[I[0][0]]

# Get the answer using the simple method
answer = simple_answer(question, relevant_text)
print("Generated Answer:", answer)

from transformers import T5ForConditionalGeneration, T5Tokenizer

# Load the T5 model and tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# Function to generate answers using T5
def get_answer_from_t5(context, question):
    input_text = f"question: {question} context: {context}"
    inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True, padding=True)
    outputs = model.generate(inputs['input_ids'], max_length=150)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# Example question
question = "What is Airtel's address?"

# Use the top retrieved chunk as context (you can adjust this based on FAISS)
relevant_text = chunks[I[0][0]]

# Get the answer from T5
answer = get_answer_from_t5(relevant_text, question)
print("Generated Answer:", answer)

!git config --global user.name "Shrishti"
!git config --global user.email "shrishtidubey35@gmail.com"

!git clone https://github.com/shri321/rag-powered-qa

from google.colab import files
uploaded = files.upload()  # Use this to upload any files you want to push